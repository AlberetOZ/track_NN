{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is python : True\n",
      "Device : cpu\n",
      "Number of actions : 4\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import pickle\n",
    "\n",
    "# создаем энвайронмент с игрой\n",
    "env = gym.make('Breakout-v0').unwrapped\n",
    "\n",
    "# настраиваем matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display    \n",
    "print(\"Is python : {}\".format(is_ipython))\n",
    "\n",
    "# выбираем девайс для игры\n",
    "DEVICE_ID = 2\n",
    "#device = torch.device('cuda:%d' % DEVICE_ID)\n",
    "device = torch.device('cpu')\n",
    "#torch.cuda.set_device(DEVICE_ID)\n",
    "print(\"Device : {}\".format(device))\n",
    "\n",
    "# запоминаем, сколько действий в игре\n",
    "ACTIONS_NUM = env.action_space.n\n",
    "print(\"Number of actions : {}\".format(ACTIONS_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 4\n",
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "MEMSIZE = 150000\n",
    "\n",
    "# Задание 1. Необходимо реализовать класс для хранения состояния игры. \n",
    "# В качестве последнего мы будем использовать состеканные 4 последовательных кадра игры.\n",
    "# Это необходимо, чтобы агент понимал скорости и ускорения игровых объектов.\n",
    "class StateHolder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.screens = []\n",
    "    \n",
    "    def appendScreen(self, screen):\n",
    "        if len(self.screens) < 4:\n",
    "            self.screens.append(screen.squeeze(0))\n",
    "        \n",
    "        else:\n",
    "            self.screens.pop(0)\n",
    "            self.screens.append(screen.squeeze(0))\n",
    "\n",
    "    def getState(self):\n",
    "        return torch.stack(self.screens)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'new_state'))\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity = MEMSIZE):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        #Положить переход в память\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        #Получить сэмпл из памяти\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2. Собрать архитектуру сети (DQN).\n",
    "# В качестве примера можно использовать сеть вида:\n",
    "# Conv(4->32) -> Conv(32->64) -> Conv(64->64) -> FC(512) -> FC(ACTIONS_NUM)\n",
    "# В качестве функций активации необходимо использовать ReLU(но совершенно не обязательно ими ограничиваться)\n",
    "# Attention : не забудьте правильно инициализировать веса, это важно для данной задачи!\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.relu1  = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.relu2  = nn.LeakyReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.relu3  = nn.LeakyReLU()\n",
    "        self.lin1   = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.lin2   = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.lin1(x.view(x.size(0), -1))\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Тут блок с кодом, генерирующим 1 кадр игры\n",
    "# Обратите внимание, что выходным тензора является torch.ByteTensor со значениями 0-255\n",
    "# Это сделанно намеренно для экономии места(4х экономия по сравнению с FloatTensor)\n",
    "# Подумайте, где и как необходимо совершать преобразование ByteTensort -> FloatTensor, чтобы его можно было подавать в сеть. \n",
    "\n",
    "def get_screen(screen):\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize( (STATE_W, STATE_H), interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "    \n",
    "    screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "    screen = screen[30:195,:]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "    return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach()\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "screen, _, _, _ = env.step(0)\n",
    "plt.imshow(get_screen(screen).cpu().reshape(-1,84).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weights.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-63e466908716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Далее стандартный метод для выбора нового действия из лекции\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpolicy_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weights.pkl'"
     ]
    }
   ],
   "source": [
    "# Далее стандартный метод для выбора нового действия из лекции\n",
    "\n",
    "with open('weights.pkl', \"rb\") as weights_file:\n",
    "    policy_net = pickle.load(weights_file).to(device)\n",
    "        \n",
    "#policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=25*1e-5)\n",
    "\n",
    "memory = ReplayMemory()\n",
    "state_holder = StateHolder()\n",
    "\n",
    "train_rewards = []\n",
    "\n",
    "mean_size = 100\n",
    "mean_step = 1\n",
    "\n",
    "def plot_rewards(rewards = train_rewards, name = \"Train\"):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(rewards)\n",
    "    # Строим график среднего вознаграждения по 100 последним эпизодам\n",
    "    if len(rewards) > mean_size:\n",
    "        means = np.array([rewards[i:i+mean_size:] for i in range(0, len(rewards) - mean_size, mean_step)]).mean(1)\n",
    "        means = np.concatenate((np.zeros(mean_size - 1), means))\n",
    "        plt.plot(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Еще немного методов из лекции\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # выбираем новый батч\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Для всех состояний считаем маску не финальнсти и конкантенируем их\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.new_state)), device=device, dtype=torch.uint8)\n",
    "\n",
    "    non_final_new_states = torch.stack([s for s in batch.new_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch  = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.action).squeeze(1).to(device)\n",
    "    reward_batch = torch.stack(batch.reward).to(device)\n",
    "    \n",
    "    # Считаем Q(s_t, a) - модель дает Q(s_t), затем мы выбираем\n",
    "    # колонки, которые соответствуют нашим действиям на щаге\n",
    "    state_action_values = policy_net(state_batch.float()).gather(1, action_batch)\n",
    "\n",
    "    # Подсчитываем ценность состяония V(s_{t+1}) для всех последующмх состояний.\n",
    "    new_state_values = torch.zeros(BATCH_SIZE, device=device).detach()\n",
    "    new_state_values[non_final_mask] = target_net(non_final_new_states.float()).max(1)[0].detach() # берем значение максимума\n",
    "    \n",
    "    # Считаем ожидаемое значение функции оценки ценности действия  Q-values\n",
    "    expected_state_action_values = (new_state_values * GAMMA) + reward_batch.squeeze(1)\n",
    "\n",
    "    # Считаем ошибку Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Оптимизация модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    del non_final_mask\n",
    "    del non_final_new_states\n",
    "    del state_batch\n",
    "    del action_batch\n",
    "    del reward_batch\n",
    "    del state_action_values\n",
    "    del new_state_values\n",
    "    del expected_state_action_values\n",
    "    del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут код для тестирования.\n",
    "# Задание 4. Просто выполнить данную ячейку и проверить вашего агента, насколько он хорош !?\n",
    "\n",
    "TEST_EPS = 0.05\n",
    "\n",
    "#with open('weights1.pkl', \"wb\") as weights_file:\n",
    "#                pickle.dump(policy_net, weights_file)\n",
    "        \n",
    "def show_state(env, step=0):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "        \n",
    "steps_done = 0\n",
    "policy_net.eval()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(4):\n",
    "        screen, _, _, _ = env.step(0)\n",
    "        state_holder.appendScreen(get_screen(screen))\n",
    "\n",
    "state = state_holder.getState()\n",
    "total_reward = 0\n",
    "\n",
    "done= False\n",
    "while not done:\n",
    "    # Выбрать и выполнить нове действие\n",
    "    steps_done += 1\n",
    "    rand = random.random()\n",
    "        \n",
    "    if rand > TEST_EPS:\n",
    "        action = policy_net(state.unsqueeze(0).float()).max(1)[1].view(1, 1)\n",
    "        \n",
    "    else:\n",
    "        action = torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    screen, reward, done, _ = env.step(action.item())\n",
    "    total_reward += reward\n",
    "    # Получаем новое состояние\n",
    "    if not done:\n",
    "        state_holder.appendScreen(get_screen(screen))\n",
    "        state = state_holder.getState()\n",
    "    else:\n",
    "        break\n",
    "    env.render()\n",
    "    #show_state(env, steps_done)\n",
    "    \n",
    "print(\"Total game reward : {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
