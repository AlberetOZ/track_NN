{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = GRU(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 1000\n",
    "optim     = SGD(rnn.parameters(), lr = 0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.69046020507812\n",
      "Clip gradient :  5.21122342772899\n",
      "69.67396545410156\n",
      "Clip gradient :  3.4631548139572144\n",
      "67.8332290649414\n",
      "Clip gradient :  2.9094699167601896\n",
      "66.17130279541016\n",
      "Clip gradient :  2.8963773712571763\n",
      "64.39301300048828\n",
      "Clip gradient :  3.2052915641575876\n",
      "62.37320327758789\n",
      "Clip gradient :  3.4616829809905925\n",
      "60.202247619628906\n",
      "Clip gradient :  3.5258077995788226\n",
      "58.022979736328125\n",
      "Clip gradient :  3.450121345995407\n",
      "55.91767501831055\n",
      "Clip gradient :  3.3291087621770723\n",
      "53.906009674072266\n",
      "Clip gradient :  3.208934162723619\n",
      "51.98323059082031\n",
      "Clip gradient :  3.098478780759649\n",
      "50.142181396484375\n",
      "Clip gradient :  2.9934307576134667\n",
      "48.379249572753906\n",
      "Clip gradient :  2.8895081567600585\n",
      "46.69285202026367\n",
      "Clip gradient :  2.7862918315976097\n",
      "45.08115768432617\n",
      "Clip gradient :  2.6847139971913307\n",
      "43.542022705078125\n",
      "Clip gradient :  2.584235434308941\n",
      "42.07416915893555\n",
      "Clip gradient :  2.483286002510278\n",
      "40.67716979980469\n",
      "Clip gradient :  2.3815157847925223\n",
      "39.35020446777344\n",
      "Clip gradient :  2.2807587761392223\n",
      "38.0908317565918\n",
      "Clip gradient :  2.1837879055838263\n",
      "36.89493179321289\n",
      "Clip gradient :  2.0926732329521114\n",
      "35.75748062133789\n",
      "Clip gradient :  2.008371129510558\n",
      "34.67319107055664\n",
      "Clip gradient :  1.9311633707918834\n",
      "33.63688659667969\n",
      "Clip gradient :  1.8609575584491496\n",
      "32.64363479614258\n",
      "Clip gradient :  1.7972943925928948\n",
      "31.689067840576172\n",
      "Clip gradient :  1.739346127015515\n",
      "30.769489288330078\n",
      "Clip gradient :  1.6860544692377974\n",
      "29.881975173950195\n",
      "Clip gradient :  1.6363320352623338\n",
      "29.02432632446289\n",
      "Clip gradient :  1.5892547132095856\n",
      "28.194854736328125\n",
      "Clip gradient :  1.5442984452443065\n",
      "27.392087936401367\n",
      "Clip gradient :  1.5015858079999482\n",
      "26.61431121826172\n",
      "Clip gradient :  1.4618144959227548\n",
      "25.85936164855957\n",
      "Clip gradient :  1.4258312119200685\n",
      "25.124698638916016\n",
      "Clip gradient :  1.394103882341624\n",
      "24.407752990722656\n",
      "Clip gradient :  1.3661158175100518\n",
      "23.706653594970703\n",
      "Clip gradient :  1.340065536581069\n",
      "23.020755767822266\n",
      "Clip gradient :  1.3137348605265902\n",
      "22.350475311279297\n",
      "Clip gradient :  1.286183297688348\n",
      "21.6962833404541\n",
      "Clip gradient :  1.2583041886218587\n",
      "21.05781364440918\n",
      "Clip gradient :  1.2321673364277317\n",
      "20.433385848999023\n",
      "Clip gradient :  1.2103556604039696\n",
      "19.81987762451172\n",
      "Clip gradient :  1.1953144987652533\n",
      "19.21319580078125\n",
      "Clip gradient :  1.187411369393145\n",
      "18.6106014251709\n",
      "Clip gradient :  1.1795537171763681\n",
      "18.015888214111328\n",
      "Clip gradient :  1.1563117423072058\n",
      "17.47056007385254\n",
      "Clip gradient :  1.4105888790189607\n",
      "17.035005569458008\n",
      "Clip gradient :  1.105450378340259\n",
      "16.644861221313477\n",
      "Clip gradient :  1.7627277770931842\n",
      "16.37942886352539\n",
      "Clip gradient :  2.1082161936859563\n",
      "16.323406219482422\n",
      "Clip gradient :  11.442870462456458\n",
      "16.357812881469727\n",
      "Clip gradient :  13.741867080866822\n",
      "16.27387046813965\n",
      "Clip gradient :  11.598698657891605\n",
      "16.18967056274414\n",
      "Clip gradient :  10.481626175375066\n",
      "16.119491577148438\n",
      "Clip gradient :  9.552009908951133\n",
      "16.055177688598633\n",
      "Clip gradient :  9.135710186700797\n",
      "15.99197006225586\n",
      "Clip gradient :  8.797965260360627\n",
      "15.928472518920898\n",
      "Clip gradient :  8.589091042918625\n",
      "15.86394214630127\n",
      "Clip gradient :  8.388192056335667\n",
      "15.798519134521484\n",
      "Clip gradient :  8.228623760558948\n",
      "15.732290267944336\n",
      "Clip gradient :  8.076892231736812\n",
      "15.665472984313965\n",
      "Clip gradient :  7.947346038494031\n",
      "15.598188400268555\n",
      "Clip gradient :  7.826077223632107\n",
      "15.530570030212402\n",
      "Clip gradient :  7.718923997542193\n",
      "15.46270751953125\n",
      "Clip gradient :  7.61833790215221\n",
      "15.39468765258789\n",
      "Clip gradient :  7.527130850146848\n",
      "15.326569557189941\n",
      "Clip gradient :  7.440761465699115\n",
      "15.258403778076172\n",
      "Clip gradient :  7.360640250825938\n",
      "15.190235137939453\n",
      "Clip gradient :  7.284400748248461\n",
      "15.12210464477539\n",
      "Clip gradient :  7.212600576680627\n",
      "15.054030418395996\n",
      "Clip gradient :  7.143669183447027\n",
      "14.986040115356445\n",
      "Clip gradient :  7.077900991476387\n",
      "14.918146133422852\n",
      "Clip gradient :  7.014369486747724\n",
      "14.850366592407227\n",
      "Clip gradient :  6.953242077875873\n",
      "14.782699584960938\n",
      "Clip gradient :  6.893730841499498\n",
      "14.715166091918945\n",
      "Clip gradient :  6.836232788059356\n",
      "14.647760391235352\n",
      "Clip gradient :  6.78007450105035\n",
      "14.580490112304688\n",
      "Clip gradient :  6.72539226271641\n",
      "14.513355255126953\n",
      "Clip gradient :  6.671787630669194\n",
      "14.4463529586792\n",
      "Clip gradient :  6.6193844965626285\n",
      "14.379478454589844\n",
      "Clip gradient :  6.567730990466477\n",
      "14.312732696533203\n",
      "Clip gradient :  6.5171142868743726\n",
      "14.246112823486328\n",
      "Clip gradient :  6.467234295047888\n",
      "14.179606437683105\n",
      "Clip gradient :  6.418007604085758\n",
      "14.1132173538208\n",
      "Clip gradient :  6.369480943587895\n",
      "14.046931266784668\n",
      "Clip gradient :  6.3214564371473\n",
      "13.980752944946289\n",
      "Clip gradient :  6.2740025671746915\n",
      "13.914667129516602\n",
      "Clip gradient :  6.227074248716952\n",
      "13.848663330078125\n",
      "Clip gradient :  6.1804867789610896\n",
      "13.782744407653809\n",
      "Clip gradient :  6.1343349964129565\n",
      "13.716901779174805\n",
      "Clip gradient :  6.088634251773641\n",
      "13.651123046875\n",
      "Clip gradient :  6.043152617392623\n",
      "13.585404396057129\n",
      "Clip gradient :  5.99808267214842\n",
      "13.519742965698242\n",
      "Clip gradient :  5.95319339379099\n",
      "13.45412826538086\n",
      "Clip gradient :  5.90866466044988\n",
      "13.388554573059082\n",
      "Clip gradient :  5.864430216425907\n",
      "13.323022842407227\n",
      "Clip gradient :  5.8204349620735405\n",
      "13.257514953613281\n",
      "Clip gradient :  5.77669555198477\n",
      "13.192039489746094\n",
      "Clip gradient :  5.733202370189817\n",
      "13.126585006713867\n",
      "Clip gradient :  5.689986532779468\n",
      "13.061147689819336\n",
      "Clip gradient :  5.647060280158197\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    cs = torch.zeros(rnn.hidden.in_features)\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqw123456789898\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "cs = torch.zeros(rnn.hidden.in_features)\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "#assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_state):\n",
    "        i = torch.sigmoid (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        f = torch.sigmoid (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        o = torch.sigmoid (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        g = torch.tanh (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        \n",
    "        state = f * prev_state + i * g\n",
    "        hidden = o * torch.tanh (state)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        upd = torch.sigmoid (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        res = torch.sigmoid (self.x2hidden(x) + self.hidden (prev_hidden))\n",
    "        \n",
    "        hidden = torch.tanh (self.x2hidden(x) + self.hidden (prev_hidden * res))\n",
    "        hidden = (torch.ones(self.hidden_size) - upd) * hidden + upd * prev_hidden\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
